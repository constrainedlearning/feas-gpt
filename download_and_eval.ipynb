{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from math import isnan \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "    \n",
    "api = wandb.Api()\n",
    "project = \"feas-ft\"\n",
    "workspace = \"hounie\"\n",
    "\n",
    "# Get our two main experiments so far\n",
    "experiment_tags = [\"LIMA\"]\n",
    "\n",
    "# get all runs that both: 1.  match any experiment tag and 2. are finished\n",
    "runs = api.runs(f\"{workspace}/{project}\",\n",
    "                {\"$and\": [\n",
    "                    {\"tags\": {\"$in\": experiment_tags}},\n",
    "                    #{\"state\": \"finished\"}\n",
    "                ]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERM\n",
      "Feas\n"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    print(run.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/meta-llama/Llama-2-7b-hf'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io = IOArgs(\n",
    "        checkpoint_dir=\"checkpoints/meta-llama/Llama-2-7b-hf\",\n",
    "        out_dir=\"out/lora/lima/dloads\",\n",
    "    )\n",
    "io.checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning as L\n",
    "from lit_gpt.args import EvalArgs, IOArgs, TrainArgs\n",
    "from lit_gpt.lora import GPT, Block, Config, lora_filter, mark_only_lora_as_trainable\n",
    "from lit_gpt.tokenizer import Tokenizer\n",
    "from lit_gpt.utils import (\n",
    "    check_valid_checkpoint_dir,\n",
    "    lazy_load,\n",
    ")\n",
    "from finetune.lora import get_longest_seq_length\n",
    "from eval.lm_eval_harness import EvalHarnessBase\n",
    "from pathlib import Path\n",
    "\n",
    "def setup(run, checkpoints =\"checkpoints/meta-llama/Llama-2-7b-hf\",\n",
    "           adapter_path = \"downloads/\", eval_tasks = [\"arc_challenge\", \"piqa\", \"hellaswag\"], num_fewshot=0):\n",
    "    config = run.config\n",
    "    io = IOArgs(\n",
    "        checkpoint_dir=Path(checkpoints),\n",
    "        out_dir=Path(\"out/lora/lima/dloads\"),\n",
    "    )\n",
    "    '''\n",
    "    train = TrainArgs(\n",
    "        save_interval=config[\"save_interval\"],\n",
    "        log_interval=config[\"log_interval\"],\n",
    "        global_batch_size=config[\"global_batch_size\"],\n",
    "        micro_batch_size=config[\"micro_batch_size\"],\n",
    "        lr_warmup_steps=config[\"lr_warmup_steps\"],\n",
    "        epochs=config[\"epochs\"],\n",
    "        epoch_size=config[\"epoch_size\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        max_seq_length=config[\"max_seq_length\"],\n",
    "        algorithm=config[\"algorithm\"],\n",
    "        dual_learning_rate=config[\"dual_learning_rate\"],\n",
    "        resilient_learning_rate=config[\"resilient_learning_rate\"],\n",
    "        resilient_alpha=config[\"resilient_alpha\"],\n",
    "        tolerance=config[\"tolerance\"],\n",
    "    ),\n",
    "    '''\n",
    "    #eval = EvalArgs(interval=config[\"interval\"], max_new_tokens=config[\"max_new_tokens\"], max_iters=config[\"max_iters\"])\n",
    "    conf = Config.from_name(\n",
    "            name=io.checkpoint_dir.name,\n",
    "            r=config[\"r\"],\n",
    "            alpha=config[\"alpha\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            to_query=config[\"to_query\"],\n",
    "            to_key=config[\"to_key\"],\n",
    "            to_value=config[\"to_value\"],\n",
    "            to_projection=config[\"to_projection\"],\n",
    "            to_mlp=config[\"to_mlp\"],\n",
    "            to_head=config[\"to_head\"],\n",
    "        )\n",
    "    fabric = L.Fabric(devices=1, strategy=\"auto\", precision=\"bf16-true\", plugins=None)\n",
    "    fabric.seed_everything(0)  # same seed for every process to init model (FSDP)\n",
    "    \n",
    "    with fabric.init_module(empty_init=(False)):\n",
    "        model = GPT(conf)\n",
    "    #tokenizer = Tokenizer(io.checkpoint_dir)\n",
    "    check_valid_checkpoint_dir(io.checkpoint_dir)\n",
    "\n",
    "    if fabric.global_rank == 0:\n",
    "        os.makedirs(io.out_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = io.checkpoint_dir / \"lit_model.pth\"\n",
    "    with lazy_load(io.checkpoint_path) as checkpoint:\n",
    "        # strict=False because missing keys due to LoRA weights not contained in state dict\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "    with lazy_load(adapter_path) as checkpoint:\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "    if len(eval_tasks)> 0:\n",
    "        eval_harness = EvalHarnessBase(\n",
    "            checkpoint_dir=str(io.checkpoint_dir),\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "    #results = eval_harness.run_eval(\n",
    "    #    eval_tasks=[\"hendrycksTest-*\"], num_fewshot=5, use_cache=False\n",
    "    #)\n",
    "    #wandb.log(results['results'])\n",
    "\n",
    "    results = eval_harness.run_eval(\n",
    "        eval_tasks=eval_tasks, num_fewshot=num_fewshot, use_cache=False\n",
    "    )\n",
    "    wandb.log(results['results'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run hounie/feas-ft/qi4gkpus (finished)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='download/qi4gkpus/out/lora/lima/lit_model_lora_finetuned.pth' mode='r' encoding='UTF-8'>\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 27.94 MiB is free. Process 1730532 has 47.17 GiB memory in use. Including non-PyTorch memory, this process has 260.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m out \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n\u001b[0;32m----> 8\u001b[0m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 56\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(run, checkpoints, adapter_path, eval_tasks, num_fewshot)\u001b[0m\n\u001b[1;32m     53\u001b[0m fabric\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# same seed for every process to init model (FSDP)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fabric\u001b[38;5;241m.\u001b[39minit_module(empty_init\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[0;32m---> 56\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#tokenizer = Tokenizer(io.checkpoint_dir)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m check_valid_checkpoint_dir(io\u001b[38;5;241m.\u001b[39mcheckpoint_dir)\n",
      "File \u001b[0;32m~/feasible-gpt/lit-gpt/lit_gpt/lora.py:507\u001b[0m, in \u001b[0;36mGPT.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpadded_vocab_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m \u001b[43mLoRALinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadded_vocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_head\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleDict(\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    517\u001b[0m         wte\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mpadded_vocab_size, config\u001b[38;5;241m.\u001b[39mn_embd),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size\n",
      "File \u001b[0;32m~/feasible-gpt/lit-gpt/lit_gpt/lora.py:121\u001b[0m, in \u001b[0;36mLoRALinear.__init__\u001b[0;34m(self, in_features, out_features, r, lora_alpha, lora_dropout, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"LoRA wrapper around linear class.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mThis class has three weight matrices:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    lora_dropout: dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(r\u001b[38;5;241m=\u001b[39mr, lora_alpha\u001b[38;5;241m=\u001b[39mlora_alpha, lora_dropout\u001b[38;5;241m=\u001b[39mlora_dropout)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Actual trainable parameters\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bab/lib/python3.11/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/bab/lib/python3.11/site-packages/lightning/fabric/utilities/init.py:57\u001b[0m, in \u001b[0;36m_EmptyInit.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.init\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/miniconda3/envs/bab/lib/python3.11/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 27.94 MiB is free. Process 1730532 has 47.17 GiB memory in use. Including non-PyTorch memory, this process has 260.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    print(run)\n",
    "    weights = run.file(\"out/lora/lima/lit_model_lora_finetuned.pth\")\n",
    "    # create the directory if it doesn't exist\n",
    "    os.makedirs(f\"download/{run.id}\", exist_ok=True)\n",
    "    out = weights.download(f\"download/{run.id}\", replace=True)\n",
    "    print(out)\n",
    "    setup(run, adapter_path=out)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
