python finetune/lora.py --precision bf16-true --lora_r 8 --train.tolerance 0.8 --train.global_batch_size 128 --io.out_dir out/lora/flan --io.train_data_dir data/flan --io.val_data_dir data/flan --train.epoch_size 1753240 --train.lr_warmup_steps 1000 --train.micro_batch_size 1 --train.algorithm erm --train.epochs 3 --train.global_batch_size 128 --train.learning_rate 0.0003 --eval.interval 5000